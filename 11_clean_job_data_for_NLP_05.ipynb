{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. define function for JSON file operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json_file(CACHE_FNAME):\n",
    "    try:\n",
    "        cache_file = open(CACHE_FNAME, 'r', encoding='utf-8-sig')\n",
    "        cache_contents = cache_file.read()\n",
    "        CACHE_DICTION = json.loads(cache_contents, encoding='utf-8-sig')\n",
    "        cache_file.close()\n",
    "        return CACHE_DICTION\n",
    "\n",
    "    except:\n",
    "        CACHE_DICTION = {}\n",
    "        return CACHE_DICTION\n",
    "    \n",
    "def dump_json_file(query_dict, file_name):\n",
    "    dumped_json_cache = json.dumps(query_dict)\n",
    "    fw = open(file_name,\"w\")\n",
    "    fw.write(dumped_json_cache)\n",
    "    fw.close()\n",
    "    print('successfully write down the file: ', file_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define function for Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):  # 檢查字元是否為英文\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def string_clean(jobDescription):  # 字串清洗\n",
    "    '''\n",
    "    input: a string (original job description)\n",
    "    output: a string (clean job description)\n",
    "    '''\n",
    "    job_desc = jobDescription.split('\\r\\n')  # 根據換行符號轉乘 List格式\n",
    "    job_words = ''\n",
    "\n",
    "    for words in job_desc:\n",
    "        words = re.sub(r'[^\\w\\s]', ' ', words)  # remove all punctuations\n",
    "        words = re.sub(r'\\d+', '', words)  # remove all numbers\n",
    "        words = words.strip()  # remove white space\n",
    "        job_words += words\n",
    "\n",
    "    return (job_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define function for 104 Job data integration (職缺資料整合成字串)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concate_clean_data(data):  # 104 職缺相關資料整合\n",
    "    '''\n",
    "    input: a dictionary of each job data (from origianl JSON format)\n",
    "    output: a string (clean job description)\n",
    "    '''\n",
    "    jobDescription = data['jobDescription']\n",
    "    jobOther = data['other']\n",
    "\n",
    "    # clean job description\n",
    "    job_words = string_clean(jobDescription) \n",
    "    job_words_other = string_clean(jobOther)  \n",
    "\n",
    "    jobDescription_clean = job_words + job_words_other  # add two string\n",
    "\n",
    "    # exclude all English job\n",
    "    if isEnglish(jobDescription_clean) == True:\n",
    "        return None\n",
    "    \n",
    "    # add jobName_clean\n",
    "    jobDescription_clean = jobDescription_clean + \" \" + data['jobName_clean']\n",
    "    \n",
    "    # add jobCategory\n",
    "    jobDescription_clean = jobDescription_clean + \" \" + data['jobCategory'].replace('／', ' ')\n",
    "\n",
    "    # add major, language, skill, certificate and specialty data if exist\n",
    "\n",
    "    def add_column_data(col_name, data, jobDescription_clean):\n",
    "        # add column data if exist\n",
    "        if len(data[col_name]) > 0:\n",
    "            for j in data[col_name]:\n",
    "                jobDescription_clean = jobDescription_clean + \" \" + j\n",
    "\n",
    "    add_column_data('major_clean', data, jobDescription_clean)\n",
    "    add_column_data('language_clean', data, jobDescription_clean)\n",
    "    add_column_data('skill_clean', data, jobDescription_clean)\n",
    "    add_column_data('certificate_clean', data, jobDescription_clean)\n",
    "    add_column_data('specialty', data, jobDescription_clean)\n",
    "\n",
    "    return jobDescription_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use Jieba for tokenization 結巴分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_cut(data, stop_words): # 定義使用結巴程式\n",
    "    '''\n",
    "    input:\n",
    "        data : string\n",
    "        stop_words: list of stopwords\n",
    "    output:\n",
    "        lst_seg : list of words after jieba\n",
    "    '''\n",
    "\n",
    "    # 使用結巴斷詞，產生 list of words\n",
    "    seg_result = jieba.cut(data, cut_all=False)\n",
    "\n",
    "    # 篩選斷詞，去掉單一中文字\n",
    "    lst_seg = []\n",
    "\n",
    "    for i in list(seg_result):\n",
    "        i = i.strip()\n",
    "        if len(i) < 1:  # 排除空值\n",
    "            continue\n",
    "        elif isEnglish(i) == False and len(i) == 1:  # 排除單一中文字\n",
    "            continue\n",
    "        elif i.isdigit() == True:  # 排除數字\n",
    "            continue\n",
    "        elif i in stop_words:  # 排除stopwords\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            lst_seg.append(i)\n",
    "\n",
    "    return lst_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_data_after_jieba(job_data, stop_words):\n",
    "    '''\n",
    "    input:\n",
    "        job_data: a dictionary of each job data (from origianl JSON format)\n",
    "        stop_words: list of stopwords\n",
    "    output: \n",
    "        str_result : a string after jieba (job content after clean and cut)\n",
    "    '''\n",
    "\n",
    "    data = concate_clean_data(job_data) # 職缺資料整合成字串\n",
    "    \n",
    "    if data != None:\n",
    "        \n",
    "        lst_seg = jieba_cut(data, stop_words) # 使用結巴斷詞\n",
    "        str_result = ' '.join(lst_seg) # 將斷詞結果整合成空白分割字串\n",
    "            \n",
    "    else:\n",
    "        str_result = ''\n",
    "\n",
    "    return str_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run function 執行程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有資料數量:  135384\n"
     ]
    }
   ],
   "source": [
    "# 1. read JSON data -------------------------------------\n",
    "\n",
    "job_data = open_json_file('json_data/test_NLP_01.json')\n",
    "print('所有資料數量: ', len(job_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\BIGDAT~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.708 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 2. Clean job Description data, 使用 Jieba 分詞 -------------------------------------\n",
    "\n",
    "jieba.load_userdict('jieba_data/Jobcontent_dict.txt')  # 指定辭典檔\n",
    "\n",
    "# 指定 Stop words檔案\n",
    "with open(file='jieba_data/Jobcontent_stopwords.txt', mode='r', encoding=\"UTF-8\") as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "    stop_words = [i.strip() for i in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將斷詞後結果存成 list of dictionary\n",
    "\n",
    "lst_jobs_clean = []\n",
    "\n",
    "for i in job_data:\n",
    "    \n",
    "    dict_job_clean = {}\n",
    "    \n",
    "    dict_job_clean['jobURL'] = i['jobURL']\n",
    "    dict_job_clean['jobName'] = i['jobName']\n",
    "    dict_job_clean['jobName_clean'] = i['jobName_clean']\n",
    "    dict_job_clean['jobCategory'] = i['jobCategory']\n",
    "    dict_job_clean['jobCat_main'] = i['jobCat_main']\n",
    "    dict_job_clean['addressRegion_clean'] = i['addressRegion_clean']\n",
    "    dict_job_clean['edu_clean'] = i['edu_clean']\n",
    "    dict_job_clean['appearDate'] = i['appearDate']\n",
    "    dict_job_clean['salary_clean'] = i['salary_clean']\n",
    "    dict_job_clean['workExp_clean'] = i['workExp_clean']\n",
    "    dict_job_clean['jobDescription_clean'] = job_data_after_jieba(i, stop_words)\n",
    "    \n",
    "    lst_jobs_clean.append(dict_job_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 輸出JSON檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully write down the file:  json_data/test_NLP_02.json\n"
     ]
    }
   ],
   "source": [
    "dump_json_file(lst_jobs_clean, 'json_data/test_NLP_02.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
